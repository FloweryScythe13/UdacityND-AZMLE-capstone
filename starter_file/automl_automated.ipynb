{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated ML\n",
    "\n",
    "TODO: Import Dependencies. In the cell below, import all the dependencies that you will need to complete the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azureml-train-automl\n",
    "%pip install azureml-widgets\n",
    "%pip install azureml-pipeline\n",
    "%pip install azureml-pipeline-steps\n",
    "%pip install kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598423888013
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, ScriptRunConfig, Environment\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.webservice.aci import AciWebservice\n",
    "from azureml.data import TabularDataset\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.pipeline.steps import AutoMLStep, PythonScriptStep\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%kaggle datasets download -d rezaunderfit/instagram-fake-and-real-accounts-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "### Overview\n",
    "TODO: In this markdown cell, give an overview of the dataset you are using. Also mention the task you will be performing.\n",
    "\n",
    "\n",
    "TODO: Get data. In the cell below, write code to access the data you will be using in this project. Remember that the dataset needs to be external."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598423890461
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "# choose a name for experiment\n",
    "experiment_name = 'udacity-instagram-users-train'\n",
    "project_folder = './automl-pipeline-project'\n",
    "experiment=Experiment(ws, experiment_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "key = \"instagram-fake-real-accounts\"\n",
    "if key in ws.datasets.keys():\n",
    "    dataset = ws.datasets[key]\n",
    "else:\n",
    "    dataset = pd.read_csv(\"./kaggle_data_ig_accounts/final-v1.csv\")\n",
    "    dataset = TabularDataset.Tabular.register_pandas_dataframe(dataframe=dataset, \n",
    "                                                               target=ws.get_default_datastore(), \n",
    "                                                               name=key, \n",
    "                                                               description=\"Kaggle dataset of Instagram accounts both real and fake.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_cluster_name = \"my-udacity-compute\"\n",
    "\n",
    "try:\n",
    "    compute_cluster = ComputeTarget(ws, compute_cluster_name)\n",
    "    print(\"Found an existing cluster, using it\")\n",
    "except ComputeTargetException as ex:\n",
    "    compute_cluster_config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_D2_V2\", max_nodes=4)\n",
    "    compute_cluster = ComputeTarget.create(ws, compute_cluster_name, compute_cluster_config)\n",
    "    \n",
    "compute_cluster.wait_for_completion(show_output=True, timeout_in_minutes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML Configuration\n",
    "\n",
    "TODO: Explain why you chose the automl settings and cofiguration you used below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Settings Rationale\n",
    "\n",
    "- task: Since the dataset schema contains a label column `is_fake` with Boolean values, a classification model is the obvious choice for training. \n",
    "- `max_concurrent_iterations`: Determined by the size of the largest VM we are permitted to use for compute in the Udacity environment\n",
    "- `primary_metric`: To get a baseline model for evaluation, I chose to optimize accuracy for the AutoML run. However, that metric \"may not optimize as well for datasets that are small, have very large class skew (class imbalance), or when the expected metric value is very close to 0.0 or 1.0\" (Microsoft Learn, \"Set up AutoML training with Python ยบ Metrics for classification scenarios\")[https://learn.microsoft.com/en-us/azure/machine-learning/v1/how-to-configure-auto-train-v1#metrics-for-classification-scenarios]. My dataset meets at least one and possibly two of these criteria (less than 800 records, and ~88% of them are labeled as fake). As such, after obtaining a baseline model and its performance metrics, I intend to run a second AutoML training job with weighted AUC as the primary metric, and then compare its performance to the first job's output.\n",
    "- `featurization`: Intentionally left as the default value `auto`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML Step Encapsulation Rationale\n",
    "\n",
    "I originally wanted to parameterize the primary_metric setting to better facilitate turning this experiment\n",
    "into a reusable pipeline for retraining the model using different metrics (such as weighted AUC). However, after struggling with type-checking issues around passing a PipelineParameter into the AutoMLConfig, I discovered that this approach is actually not supported by Azure ML for use in an AutoMLStep: https://stackoverflow.com/a/62382471\n",
    "\n",
    "It took a couple days, but I eventually succeeded in implementing the workaround suggested by the Microsoft AML product group. My pipeline now contains a PythonScriptStep rather than an AutoMLStep, and the Python script receives a pipeline parameter value for the `primary_metric`, generates an AutoMLConfig object using that input, and generates an AutoML job as a child run of the pipeline. The only drawback of this approach is that the programmatic submission of an AutoML run by the script requires interactive authentication - once the step has started, the user must open the details window for the step, navigate to the `user_logs/std_log.txt` log file in the Outputs tab, and follow the instruction there by opening a new browser tab to https://microsoft.com/devicelogin and entering the login code therein. Removing this interactive authentication requirement requires the creation of an Azure Service Principal; because the Udacity lab environment prevents oneself from creating a Service Principal, this piece is left unimplemented here.\n",
    "\n",
    "However, although it has run successfully at least once, it has not done so reliably; missing package version dependencies in the PythonScriptStep's Conda environment continue to plague it with runtime exceptions, with the specific packages required appearing to change from one run of the pipeline to the next. Since this has become an unproductive use of valuable time, I am thus stopping further work on this implementation, but have left the code for it here for review and posterity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline, PipelineData, TrainingOutput\n",
    "from azureml.pipeline.core import PipelineParameter\n",
    "\n",
    "automl_pipeline_param_primarymetric = PipelineParameter(name=\"primary_metric\", default_value=\"accuracy\")\n",
    "\n",
    "ds = ws.get_default_datastore()\n",
    "metrics_output_name = 'metrics_output'\n",
    "best_model_output_name = 'best_model_output'\n",
    "\n",
    "metrics_data = PipelineData(name='metrics_data',\n",
    "                           datastore=ds,\n",
    "                           pipeline_output_name=metrics_output_name,\n",
    "                           training_output=TrainingOutput(type='Metrics'))\n",
    "model_data = PipelineData(name='model_data',\n",
    "                           datastore=ds,\n",
    "                           pipeline_output_name=best_model_output_name,\n",
    "                           training_output=TrainingOutput(type='Model'))\n",
    "\n",
    "# automl_step = AutoMLStep(\n",
    "#     name = 'automl_module', \n",
    "#     automl_config = automl_config,\n",
    "#     outputs = [metrics_data, model_data],\n",
    "#     allow_reuse = True\n",
    "# )\n",
    "\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "script_env = Environment.from_conda_specification(name=\"ig-pipeline-env\", file_path=\"./conda_dependencies.yml\")\n",
    "script_env.python.user_managed_dependencies = False\n",
    "\n",
    "#script_env.register(workspace=ws)\n",
    "\n",
    "# set up script run configuration\n",
    "script_config = ScriptRunConfig(\n",
    "    source_directory='.\\train_scripts',\n",
    "    script='automl_train_script.py',\n",
    "    compute_target=compute_cluster,\n",
    "    environment=script_env\n",
    ")\n",
    "\n",
    "\n",
    "script_step = PythonScriptStep(name=\"train script step\", \n",
    "                                script_name='automl_train_script.py', \n",
    "                                source_directory=\"./train_scripts\",\n",
    "                                compute_target=compute_cluster,\n",
    "                                runconfig=script_config.run_config,\n",
    "                                # outputs = [metrics_data, model_data],\n",
    "                                arguments=[\"--primary_metric\", automl_pipeline_param_primarymetric],\n",
    "                                allow_reuse=True\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431107951
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: After submitting, you must open the script step in Designer and perform interactive authentication!!\n",
    "automl_pipeline = Pipeline(workspace=ws, description=\"An automated ML pipeline for training a classifier to detect fake IG accounts\", steps = [script_step])\n",
    "pipeline_run = experiment.submit(automl_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Details\n",
    "\n",
    "OPTIONAL: Write about the different models trained and their performance. Why do you think some models did better than others?\n",
    "\n",
    "TODO: In the cell below, use the `RunDetails` widget to show the different experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431121770
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model\n",
    "\n",
    "TODO: In the cell below, get the best model from the automl experiments and display all the properties of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431425670
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "for step in pipeline_run.get_steps():\n",
    "    print(\"Outputs of step \" + step.name)\n",
    "    \n",
    "    # Get a dictionary of StepRunOutputs with the output name as the key \n",
    "    output_dict = step.get_outputs()\n",
    "    \n",
    "    for name, output in output_dict.items():\n",
    "        \n",
    "        output_reference = output.get_port_data_reference() # Get output port data reference\n",
    "        print(\"\\tname: \" + name)\n",
    "        print(\"\\tdatastore: \" + output_reference.datastore_name)\n",
    "        print(\"\\tpath on datastore: \" + output_reference.path_on_datastore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = pipeline_run.find_step_run('train script step')\n",
    "\n",
    "if train_step:\n",
    "    train_step_obj = train_step[0] # since we have only one step by name 'train.py'\n",
    "    train_step_obj.get_output_data('metrics_data').download(\"./outputs\") # download the output to current directory\n",
    "    train_step_obj.get_output_data('model_data').download(\"./outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431426111
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#TODO: Save the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "\n",
    "Remember you have to deploy only one of the two models you trained but you still need to register both the models. Perform the steps in the rest of this notebook only if you wish to deploy this model.\n",
    "\n",
    "TODO: In the cell below, register the model, create an inference config and deploy the model as a web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598431435189
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598431657736
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "TODO: In the cell below, send a request to the web service you deployed to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598432707604
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598432765711
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "TODO: In the cell below, print the logs of the web service and delete the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submission Checklist**\n",
    "- I have registered the model.\n",
    "- I have deployed the model with the best accuracy as a webservice.\n",
    "- I have tested the webservice by sending a request to the model endpoint.\n",
    "- I have deleted the webservice and shutdown all the computes that I have used.\n",
    "- I have taken a screenshot showing the model endpoint as active.\n",
    "- The project includes a file containing the environment details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "c347c8f9a7ef94e4c9e03b4513be7835ed18f45b99a2a817fb579f408b867b16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
